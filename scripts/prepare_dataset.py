"""
Skrypt do jednorazowego przygotowania i podziaÅ‚u datasetu HAM10000.

Tworzy plik `ham10000_splits.csv` z podziaÅ‚em na zbiory
treningowy, walidacyjny i testowy, co zapewnia spÃ³jnoÅ›Ä‡
i przyspiesza Å‚adowanie danych w gÅ‚Ã³wnym skrypcie.
"""
import os
import sys
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from tqdm import tqdm

# Dodaj Å›cieÅ¼kÄ™ do src
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from data.dataset_downloader import download_ham10000

def find_image_paths(data_dir: str, metadata: pd.DataFrame) -> pd.DataFrame:
    """Znajduje i mapuje Å›cieÅ¼ki do obrazÃ³w dla kaÅ¼dego image_id."""
    print("ğŸ—ºï¸ Mapowanie Å›cieÅ¼ek do obrazÃ³w...")
    
    path_map = {}
    image_dirs = ['HAM10000_images_part_1', 'HAM10000_images_part_2']
    
    for folder in image_dirs:
        folder_path = os.path.join(data_dir, folder)
        if not os.path.isdir(folder_path):
            print(f"âš ï¸ OstrzeÅ¼enie: Folder '{folder_path}' nie istnieje.")
            continue
        
        for image_file in tqdm(os.listdir(folder_path), desc=f"Przeszukiwanie {folder}"):
            if image_file.endswith('.jpg'):
                image_id = os.path.splitext(image_file)[0]
                path_map[image_id] = os.path.join(folder, image_file)
                
    metadata['path'] = metadata['image_id'].map(path_map)
    
    missing_paths = metadata['path'].isnull().sum()
    if missing_paths > 0:
        print(f"âŒ Nie znaleziono {missing_paths} obrazÃ³w! Upewnij siÄ™, Å¼e dataset jest kompletny.")
    else:
        print("âœ… PomyÅ›lnie zmapowano wszystkie obrazy.")
        
    return metadata.dropna(subset=['path'])

def create_splits(data_dir: str = 'data', 
                  val_size: float = 0.15, 
                  test_size: float = 0.15, 
                  random_state: int = 42):
    """
    Tworzy stratyfikowany podziaÅ‚ danych i zapisuje go do pliku CSV.
    Automatycznie pobiera dataset jeÅ›li nie istnieje.
    """
    metadata_path = os.path.join(data_dir, 'HAM10000_metadata.csv')
    output_path = os.path.join(data_dir, 'ham10000_splits.csv')
    
    # SprawdÅº czy dane juÅ¼ istniejÄ…
    if not os.path.exists(metadata_path):
        print(f"âŒ Nie znaleziono pliku metadanych: {metadata_path}")
        print("ğŸ“¥ PrÃ³bujÄ™ automatycznie pobraÄ‡ dataset HAM10000...")
        
        try:
            success = download_ham10000(data_dir)
            if not success:
                print("âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ datasetu!")
                print("   SprawdÅº konfiguracjÄ™ Kaggle API w pliku .env")
                return
            print("âœ… Dataset zostaÅ‚ pobrany pomyÅ›lnie!")
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas pobierania datasetu: {e}")
            print("   SprawdÅº konfiguracjÄ™ Kaggle API w pliku .env")
            return

    print("ğŸ“– Wczytywanie metadanych...")
    metadata = pd.read_csv(metadata_path)
    
    # Krok 1: Mapowanie Å›cieÅ¼ek
    metadata = find_image_paths(data_dir, metadata)
    
    # Przygotowanie do podziaÅ‚u
    labels = metadata['dx'] # 'dx' to kolumna z etykietami klas
    
    # Krok 2: PodziaÅ‚ na train i (val + test)
    print(f"ğŸ”ª Dzielenie danych (test={test_size}, val={val_size})...")
    
    # Najpierw oddzielamy zbiÃ³r testowy
    splitter_test = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)
    train_val_idx, test_idx = next(splitter_test.split(metadata, labels))
    
    # Stworzenie ramek danych
    train_val_set = metadata.iloc[train_val_idx]
    test_set = metadata.iloc[test_idx]
    
    # Przypisanie splitu
    metadata['split'] = 'test'
    metadata.iloc[train_val_idx, metadata.columns.get_loc('split')] = 'train' # DomyÅ›lnie train

    # Krok 3: PodziaÅ‚ (train + val) na train i val
    val_size_adjusted = val_size / (1.0 - test_size)
    labels_train_val = train_val_set['dx']
    
    splitter_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size_adjusted, random_state=random_state)
    train_idx, val_idx = next(splitter_val.split(train_val_set, labels_train_val))
    
    # Indeksy w oryginalnej ramce danych
    original_train_idx = train_val_set.iloc[train_idx].index
    original_val_idx = train_val_set.iloc[val_idx].index
    
    # Aktualizacja splitu
    metadata.loc[original_train_idx, 'split'] = 'train'
    metadata.loc[original_val_idx, 'split'] = 'val'
    
    # Zapis do pliku
    final_df = metadata[['image_id', 'dx', 'split', 'path']]
    final_df.to_csv(output_path, index=False)
    
    print(f"âœ… PodziaÅ‚ danych zapisany w: {output_path}")
    print("\nğŸ“Š RozkÅ‚ad prÃ³bek w podziaÅ‚ach:")
    print(final_df['split'].value_counts())

if __name__ == '__main__':
    create_splits() 